{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Topic Model for NLP using LDA\n",
    "following tutorial of Susan Li at https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/fb/c0cefcecf82b445ff2a714935db5b475a25202d6b63241c7e95ca004136a/gensim-3.7.3-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 24kB/s  eta 0:00:01    75% |████████████████████████        | 18.5MB 13.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /anaconda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/ba/7eaf3c0dbe601c43d88e449dcd7b61d385fe07c0167163f63f58ece7c1b5/smart_open-1.8.3.tar.gz (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /anaconda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: requests in /anaconda/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/9c/7111470f07700a6b06305943fc7521e49d9669dbda0c1862c4658130f235/boto3-1.9.146-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.146 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/4e/73751a9041fe4d6bb4f9635d74de21d6267d27067593cea370e9d0d7a043/botocore-1.12.146-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 87kB/s  eta 0:00:01    60% |███████████████████▎            | 3.3MB 17.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /anaconda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.146->boto3->smart-open>=1.7.0->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.146->boto3->smart-open>=1.7.0->gensim)\n",
      "Collecting urllib3<1.25,>=1.20; python_version >= \"3.4\" (from botocore<1.13.0,>=1.12.146->boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/Weiwei/Library/Caches/pip/wheels/b8/cb/43/c0ba52baf2b0e371ec1d5b2d4685d6d24617b1391f3eeacda5\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, urllib3, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.146 botocore-1.12.146 gensim-3.7.3 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.3 urllib3-1.24.3\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all needed libraries\n",
    "import pandas as pd\n",
    "#import gensim\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer as wnLemmatizer\n",
    "#from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The original dataset abcnews-date-text.csv has over 1 million rows and is over 50Mb. To reduce computation workload and storing data in github, I am taking only the first 50k rows of the data and use it for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103663, 2)\n"
     ]
    }
   ],
   "source": [
    "# if running this implementation with shrinked data\n",
    "# this cell is not needed\n",
    "#data = pd.read_csv('abcnews-date-text.csv',error_bad_lines=False)\n",
    "#print(data.shape)\n",
    "#data = data[:500000]\n",
    "#data.to_csv('abcnews-short.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "read the dataset as documents and a preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "                                            headline\n",
      "0  aba decides against community broadcasting lic...\n",
      "1     act fire witnesses must be aware of defamation\n",
      "2     a g calls for infrastructure protection summit\n",
      "3           air nz staff in aust strike for pay rise\n",
      "4      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('abcnews-short.csv',error_bad_lines=False)\n",
    "documents = data[['headline_text']]\n",
    "documents.columns = ['headline']\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to lemmatize and stemming words\n",
    "def lem_stem(text):\n",
    "    return nltk.stem.WordNetLemmatizer().lemmatize(text,pos='v')\n",
    "\n",
    "#function of preprocessing\n",
    "#filter stopwords and words less than 3 characters, then lem&stem\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text,min_len=4):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            result.append(lem_stem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document:  rain helps dampen bushfires\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['rain', 'help', 'dampen', 'bushfires']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents.iat[4310,0]\n",
    "print('original document: ',doc_sample)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [decide, community, broadcast, licence]\n",
       "1                         [witness, aware, defamation]\n",
       "2           [call, infrastructure, protection, summit]\n",
       "3                          [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travellers]\n",
       "5               [ambitious, olsson, win, triple, jump]\n",
       "6               [antic, delight, record, break, barca]\n",
       "7    [aussie, qualifier, stosur, waste, memphis, ma...\n",
       "8             [aust, address, security, council, iraq]\n",
       "9                         [australia, lock, timetable]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 community\n",
      "2 decide\n",
      "3 licence\n",
      "4 aware\n",
      "5 defamation\n",
      "6 witness\n",
      "7 call\n",
      "8 infrastructure\n",
      "9 protection\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.corpora.dictionary.Dictionary"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(110, 1), (480, 1), (946, 1), (4132, 1)]\n",
      "help\n",
      "rain\n",
      "bushfires\n",
      "dampen\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(bow_corpus[4310])\n",
    "#and translate back to word with the dictionary\n",
    "for i in range(len(bow_corpus[4310])):\n",
    "    print(dictionary[bow_corpus[4310][i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing function:\n",
      " ['rain', 'help', 'dampen', 'bushfires']\n"
     ]
    }
   ],
   "source": [
    "#writing my own function to preprocess text\n",
    "def preProcess(originalString,lowerCase=True,minLength=4):\n",
    "    if lowerCase:\n",
    "        originalString = originalString.lower()\n",
    "    words = []\n",
    "    for word in originalString.split(' '):\n",
    "        if len(word) >= minLength:\n",
    "            if word not in STOPWORDS:\n",
    "                words.append(wnLemmatizer().lemmatize(word,pos='v'))\n",
    "    return words\n",
    "\n",
    "print('testing function:\\n',preProcess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [decide, community, broadcast, licence]\n",
       "1                         [witness, aware, defamation]\n",
       "2           [call, infrastructure, protection, summit]\n",
       "3                          [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travellers]\n",
       "5               [ambitious, olsson, win, triple, jump]\n",
       "6               [antic, delight, record, break, barca]\n",
       "7    [aussie, qualifier, stosur, waste, memphis, ma...\n",
       "8             [aust, address, security, council, iraq]\n",
       "9                         [australia, lock, timetable]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = documents['headline_text'].map(preProcess)\n",
    "tokenized_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
